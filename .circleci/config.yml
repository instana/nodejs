# See https://circleci.com/docs/2.0/language-javascript/ for docs.

version: 2.1

parameters:
  workflow:
    type: string
    default: build
  executors:
    type: integer
    default: 2

shared: &shared
  working_directory: ~/repo

  environment:
    - ELASTICSEARCH=127.0.0.1:9200
    - MONGODB=127.0.0.1:27017
    - ZOOKEEPER=127.0.0.1:2181
    - KAFKA=127.0.0.1:9092
    - KAFKA2=127.0.0.1:29092
    - SCHEMA_REGISTRY=127.0.0.1:8081
    - REDIS=127.0.0.1:6379
    - MYSQL_HOST=127.0.0.1
    - MYSQL_PORT=3306
    - MYSQL_USER=node
    - MYSQL_PW=nodepw
    - MYSQL_DB=nodedb
    - POSTGRES_HOST=127.0.0.1
    - POSTGRES_USER=node
    - POSTGRES_PASSWORD=nodepw
    - POSTGRES_DB=nodedb
    - MEMCACHED=localhost:11211
    - MSSQL_HOST=localhost
    - MSSQL_PORT=1433
    - MSSQL_USER=sa
    - MSSQL_PW=stanCanHazMsSQL1

  steps:
    - checkout

    # The main intent for skipping the build is the other-nodejs-versions workflow which is built daily. If no new
    # commits have been merged to main, we do not need to built the same commit over and over again.
    #
    # Theoretically we could also save the build on the main branch directly after merging a PR - since we always
    # require a branch that has been rebased on main, and require a successful build before merging, we always have
    # built the same commit on the PR branch already before merging, and then we build the commit again on main after
    # merging to main. But there is a little kink here: When merging the PR on Github via the UI, the commit on main
    # will have a different hash than the commit on the branch, even if the branch was up to date and a fast-forward
    # merge would have been possible.
    #
    # See https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/incorporating-changes-from-a-pull-request/about-pull-request-merges#rebase-and-merge-your-commits:
    # > [...] In that way, the rebase and merge behavior resembles a fast-forward merge by maintaining a linear project
    # > history. However, rebasing achieves this by re-writing the commit history on the base branch with new commits.
    #
    # Of course you could always do an actual git merge --ff-only on the main branch locally and push that to main to
    # avoid that, if you really wanted to.
    - run:
        name: Skip Build If This Commit Has Already Been Built
        command: |
          if [[ $FORCE_REBUILT != true ]]; then
            if bin/check-circle-ci-status.js check-for-successful-build-on-any-branch; then
              echo "Commit has been built already, skipping remaining build steps."
              circleci step halt
            fi
          fi

    - run: node --version

    - run:
        name: Optionally Install A Specific npm Version
        command: |
          if [ -n "$NPM_VERSION" ]; then
            sudo npm install npm@$NPM_VERSION -g
          fi

    - run:
        name: Optionally Install A Specific node-gyp Version
        command: |
          if [ -n "$NODE_GYP_VERSION" ]; then
            npm install node-gyp@$NODE_GYP_VERSION
          fi

    - run: npm --version

    - restore_cache:
        keys:
          - v{{ .Environment.CACHE_VERSION }}-root-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "package-lock.json" }}
    - restore_cache:
        keys:
          - v{{ .Environment.CACHE_VERSION }}-babel-typescript-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "packages/collector/test/apps/babel-typescript/package-lock.json" }}
    - restore_cache:
        keys:
          - v{{ .Environment.CACHE_VERSION }}-prisma-app-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "packages/collector/test/tracing/database/prisma/package-lock.json" }}
    - restore_cache:
        keys:
          - v{{ .Environment.CACHE_VERSION }}-misc-test-durations-{{ .Environment.CIRCLE_JOB }}-{{ checksum "misc/test-durations/package-lock.json" }}

    # We do not use npm ci (short for clean install) because it removes the existing node_modules folder. But we want to
    # re-use the existing node_modules folders from CircleCI's cache. Using npm ci would defeat that purpose. For the
    # same purpose, we reset the package lock file after running npm install. Sometimes npm install updates the lock
    # file and that will also break caching because its checksum would then change between save_cache and restore_cache
    # in the next CI run.
    - run:
        name: npm install
        command: |
          date
          npm install
          git checkout package-lock.json

    - run:
        name: install dependencies of test-durations script
        command: |
          cd misc/test-durations
          npm install
          git checkout package-lock.json

    # Cache the node_modules folders for the next run (before potentially installing other module versions like grpc,
    # see below).
    - save_cache:
        paths:
          - node_modules
        key: v{{ .Environment.CACHE_VERSION }}-root-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "package-lock.json" }}
    - save_cache:
        paths:
          - misc/test-durations/node_modules
        key: v{{ .Environment.CACHE_VERSION }}-misc-test-durations-{{ .Environment.CIRCLE_JOB }}-{{ checksum "misc/test-durations/package-lock.json" }}

    # Only run audit with the most recent Node.js version - one Node version is enough.
    - run:
        name: Execute npm audit
        command: "([[ $(node -v) =~ ^v18.*$ ]] && npm run audit) || [[ ! $(node -v) =~ ^v18.*$ ]]"

    # Check if all commits since the last release comply with the format that conventional-commit understands. This
    # makes sure that lerna version (with the "conventionalCommits" option set in lerna.json) can correctly determin the
    # next version number and generate changelogs.
    - run:
        name: Execute commitlint
        command: "([[ $(node -v) =~ ^v18.*$ ]] && node_modules/.bin/commitlint --from $(git describe --tags --abbrev=0)) || [[ ! $(node -v) =~ ^v18.*$ ]]"

    # Run linting first, we don't want to wait ages for the test suite to finish only to end the CI job with a linting
    # error.
    - run:
        name: Lint Source Code
        command: npm run lint

    # Check for unused dependencies in package.json files.
    # We might get rid of this at some point if
    # https://github.com/import-js/eslint-plugin-import/issues/877
    # is ever implemented
    - run:
        name: Execute depcheck
        # Note: We explicitly exclude a couple of packages from the depcheck check:
        # - nan: is used in packages/autoprofile/binding.gyp. Apparently depcheck does not understand binding.gyp files.
        # - agent-base: See commit comment:
        #   https://github.com/instana/nodejs/commit/84b597773f94b4a6b18356652396c92cc16ea4c3
        # - @types/*: These packages are installed to provide additional types that tsc uses, but we never explicitly
        #   require those packages, thus they get flagged as false positives by depcheck.
        command: "([[ $(node -v) =~ ^v18.*$ ]] && npm run depcheck) || [[ ! $(node -v) =~ ^v18.*$ ]]"

    # Clean up old left over AWS resources from earlier test runs (created more more than 3 hours ago):
    # Do not execute the script on every parallel build
    - run:
        name: Clean Up AWS Resources
        command: |
          if [[ $(node -v) =~ ^v18.*$ && $CIRCLE_NODE_INDEX == "0" ]]; then
            bin/clean-aws.js --service=s3
            bin/clean-aws.js --service=dynamodb
            bin/clean-aws.js --service=sqs
            bin/clean-aws.js --service=kinesis
          fi

    # Run the test suites.
    # The ci_glob_setup.sh script determines which tests will be run on each CircleCI executor.
    - run:
        name: Run Tests
        command: |
          source bin/ci_glob_setup.sh
          date
          if [[ -z "$NODE_COVERAGE" ]]; then
            if [[ -z "$COLLECTOR_ONLY" ]]; then
              npm run test:ci
            else
              npm run test:ci:collector
            fi
          else
            npm run coverage-all
          fi

    - store_test_results:
        path: test-results

    - run:
        name: Analyze Test Durations
        command: node misc/test-durations/ci
    # Generate ESLint JSON report for Sonarqube.
    - run:
        name: Run ESLint and generate ESLint report
        command: |
          if [ -n "$NODE_COVERAGE" ]; then
            npx eslint packages/ -f json > eslint-report.json || exit 0
          fi
    # Execute the SonarScanner to analyze the project
    - run:
        name: Run SonarQube Scanner
        command: |
          if [ -n "$NODE_COVERAGE" ]; then
            npx sonarqube-scanner
          fi

    - store_artifacts:
        path: coverage
    - store_artifacts:
        path: test-results
    - store_artifacts:
        path: test-duration-breakdown

    - save_cache:
        paths:
          - packages/collector/test/apps/babel-typescript/node_modules
        key: v{{ .Environment.CACHE_VERSION }}-babel-typescript-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "packages/collector/test/apps/babel-typescript/package-lock.json" }}
    - save_cache:
        paths:
          - packages/collector/test/tracing/database/prisma/node_modules
        key: v{{ .Environment.CACHE_VERSION }}-prisma-app-dependencies-{{ .Environment.CIRCLE_JOB }}-{{ checksum "packages/collector/test/tracing/database/prisma/package-lock.json" }}

elasticsearch: &elasticsearch
  - image: docker.elastic.co/elasticsearch/elasticsearch:8.6.1
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms500m -Xmx500m"
      - xpack.security.enabled=false

kafka: &kafka
  - image: wurstmeister/kafka:2.12-2.2.1
    environment:
      KAFKA_LISTENERS: EXTERNAL://:9092,PLAINTEXT://:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://127.0.0.1:29092,EXTERNAL://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CREATE_TOPICS: "test:1:1,test-topic-1:1:1,test-topic-2:1:1,test-batch-topic-1:1:1,test-batch-topic-2:1:1,rdkafka-topic:1:1,kafka-avro-topic:1:1"
      KAFKA_ZOOKEEPER_CONNECT: 127.0.0.1:2181

couchbase: &couchbase
  - image: couchbase/server:7.1.4
    name: couchbase

couchbase-setup: &couchbase-setup
  image: amd64/centos:7
  name: couchbase-setup
  entrypoint:
    - "bash"
    - "-ecx"
    - |
        sleep 30 && \
        curl -v -X POST http://couchbase:8091/pools/default -d memoryQuota=512 -d indexMemoryQuota=512 && \
        curl -v http://couchbase:8091/node/controller/setupServices -d services=kv%2Ceventing%2Cindex%2Cn1ql%2Ccbas%2Cfts && \
        curl -v http://couchbase:8091/settings/web -d port=8091 -d username=node -d password=nodepwd && \
        curl -i -u node:nodepwd -X POST http://couchbase:8091/settings/indexes -d 'storageMode=memory_optimized' && \
        curl -v -u node:nodepwd -X POST http://couchbase:8091/pools/default/buckets -d name=projects -d bucketType=couchbase -d ramQuota=128 -d flushEnabled=1 && \
        curl -v -u node:nodepwd -X POST http://couchbase:8091/pools/default/buckets -d name=companies -d bucketType=ephemeral -d ramQuota=128 -d flushEnabled=1
        tail -f /dev/null

localstack: &localstack
  image: localstack/localstack:latest
  name: localstack

schema-registry: &schema-registry
  - image: confluentinc/cp-schema-registry:4.1.0
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "127.0.0.1:2181"
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: "PLAINTEXT://127.0.0.1:29092"
      SCHEMA_REGISTRY_HOST_NAME: schema-registry

mongo: &mongo
  - image: circleci/mongo:4.1.13

mssql: &mssql
  - image: mcr.microsoft.com/mssql/server:2022-RC1-ubuntu-20.04
    environment:
      ACCEPT_EULA: Y
      SA_PASSWORD: stanCanHazMsSQL1
      MSSQL_PID: Express

mysql: &mysql
  - image: circleci/mysql:8.0.1
    environment:
      MYSQL_ROOT_PASSWORD: nodepw
      MYSQL_DATABASE: nodedb
      MYSQL_USER: node
      MYSQL_PASSWORD: nodepw
      MYSQL_ROOT_HOST: 0.0.0.0

postgres: &postgres
  - image: circleci/postgres:10.9-alpine
    environment:
      POSTGRES_USER: node
      POSTGRES_PASSWORD: nodepw
      POSTGRES_DB: nodedb

rabbitmq: &rabbitmq
  - image: circleci/rabbitmq:3.7.7

redis: &redis
  - image: circleci/redis:5.0.14

zookeeper: &zookeeper
  - image: wurstmeister/zookeeper:latest

nats: &nats
  - image: nats:2.9.14-linux

nats-streaming: &nats-streaming
  - image: nats-streaming:0.25.3-linux
    command: -p 4223 -m 8223

memcached: &memcached
  - image: memcached:1.6.9

jobs:
  "node-20":
    environment:
      NODE_GYP_VERSION: 8.4.1
    docker:
      - image: cimg/node:20.9
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: 15
    resource_class: large

  "node-18":
    docker:
      - image: cimg/node:18.18
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: 15
    resource_class: medium+

  "node-16":
    docker:
      - image: cimg/node:16.20
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: 15
    resource_class: medium+

  "node-18-esm":
    environment:
      RUN_ESM: true
      COLLECTOR_ONLY: true
    docker:
      - image: cimg/node:18.18
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: << pipeline.parameters.executors >>
    resource_class: medium+

  "node-14":
    environment:
      NPM_VERSION: 8.6.0
    docker:
      - image: cimg/node:14.21
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: 15
    resource_class: medium+

  "node-coverage":
    environment:
        NODE_COVERAGE: true
    docker:
      - image: cimg/node:18.18
      - <<: *zookeeper
      - <<: *elasticsearch
      - <<: *mongo
      - <<: *redis
      - <<: *kafka
      - <<: *schema-registry
      - <<: *mysql
      - <<: *postgres
      - <<: *mssql
      - <<: *rabbitmq
      - <<: *nats
      - <<: *nats-streaming
      - <<: *memcached
      - <<: *couchbase
      - <<: *couchbase-setup
      - <<: *localstack
    <<: *shared
    parallelism: 1
    resource_class: large

workflows:
  version: 2

  build:

    when:
      and:

        # This makes sure the "Trigger Pipeline" button when started with a specific workflow parameter value
        # (say, "coverage) does not also start the "build" workflow.
        - equal: [ build, << pipeline.parameters.workflow >> ]

        # This makes sure that none of our triggers
        # (https://app.circleci.com/settings/project/github/instana/nodejs/triggers)
        # kicks off the main build workflow jobs. (This workflow is triggered by the source "webhook", that is, by
        # pushing commits to Github branches for which a PR has been opened.)
        - not:
          equal: [ scheduled_pipeline, << pipeline.trigger_source >> ]
    jobs:
      - "node-18"
      - "node-16"
      - "node-14"

  other-nodejs-versions:
    when:
      or:
        # See https://app.circleci.com/settings/project/github/instana/nodejs/triggers.
        - equal: [ nightly-build-2-am, << pipeline.schedule.name >> ]
        # This allows us to kick off this workflow on demand via the CircleCI UI with the "Trigger Pipeline" button,
        # by providing the parameter "workflow: other-nodejs-versions"
        - equal: [ other-nodejs-versions, << pipeline.parameters.workflow >> ]
    jobs:
      - "node-20"
      - "node-16"
      - "node-14"

  esm-build:
    when:
      or:
        # See https://app.circleci.com/settings/project/github/instana/nodejs/triggers.
        - equal: [ nightly-build-4-am, << pipeline.schedule.name >> ]
        # This allows us to kick off this workflow on demand via the CircleCI UI with the "Trigger Pipeline" button,
        # by providing the parameter "workflow: esm-build"
        - equal: [ esm-build, << pipeline.parameters.workflow >> ]
    jobs:
      - "node-18-esm"

  coverage:
    when:
      or:
        # See https://app.circleci.com/settings/project/github/instana/nodejs/triggers.
        - equal: [ weekly-build-sunday-0-am, << pipeline.schedule.name >> ]
        # This allows us to kick off this workflow on demand via the CircleCI UI with the "Trigger Pipeline" button,
        # by providing the parameter "workflow: coverage"
        - equal: [ coverage, << pipeline.parameters.workflow >> ]
    jobs:
      - "node-coverage"
